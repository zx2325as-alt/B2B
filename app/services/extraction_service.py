import json
import re
import time
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from app.core.config import settings
from app.services.llm import llm_service
from app.models.sql_models import Scenario, Character
from app.utils.logger import logger
from app.services.character_observation_service import character_observation_service
from app.services.character_service import character_service

class ExtractionService:
    async def detect_scenario(self, text: str, history: List[Dict], available_scenarios: List[Scenario]) -> Optional[int]:
        """
        Identify the most likely scenario from the available list based on conversation.
        Returns the Scenario ID.
        """
        if not available_scenarios:
            return None

        scenario_descriptions = [f"ID {s.id}: {s.name} - {s.description}" for s in available_scenarios]
        
        # Load config (Moved to deep_analysis.scenario_detection)
        config = settings.PROMPTS.get("deep_analysis", {}).get("scenario_detection", {})
        prompt_template = config.get("prompt", "")
        temperature = config.get("temperature", 0.1)
        
        prompt = prompt_template.format(
            scenario_descriptions=chr(10).join(scenario_descriptions),
            text=text
        )
        
        response = await llm_service.chat_completion(
            [{"role": "user", "content": prompt}],
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        try:
            result = json.loads(response)
            return result.get("scenario_id")
        except Exception as e:
            logger.error(f"Scenario detection failed: {e}")
            return None

    async def extract_character_info(self, text: str, character_name: str, existing_attributes: Dict) -> Dict[str, Any]:
        """
        Extract new information about a character from the text to update their profile.
        """
        # Load config (Moved to deep_analysis.character_info)
        config = settings.PROMPTS.get("deep_analysis", {}).get("character_info", {})
        prompt_template = config.get("prompt", "")
        temperature = config.get("temperature", 0.1)

        prompt = prompt_template.format(
            character_name=character_name,
            existing_attributes=json.dumps(existing_attributes),
            text=text
        )
        
        response = await llm_service.chat_completion(
            [{"role": "user", "content": prompt}],
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except Exception as e:
            logger.error(f"Character info extraction failed: {e}")
            return {}

    async def process_analysis_results(self, db: Session, session_id: str, structured_data: Dict[str, Any]):
        """
        å¤„ç†ç»“æ„åŒ–åˆ†æç»“æœ (Process Structured Analysis Results).
        
        åŠŸèƒ½æè¿°:
        å½“ `deep_analyze` ç”Ÿæˆç»“æ„åŒ–æ•°æ®åï¼Œæ­¤æ–¹æ³•è´Ÿè´£å°†å…¶è½¬åŒ–ä¸ºç³»ç»Ÿå†…éƒ¨çš„æŒä¹…åŒ–æ•°æ®ã€‚
        è¿™æ„æˆäº†ç³»ç»Ÿçš„â€œä¸»åŠ¨æ„ŸçŸ¥â€èƒ½åŠ›ã€‚
        
        åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ± (Three Pillars):
        1. **äº‹ä»¶ç”Ÿæˆ (Event Generation)**: (ä»£ç ä¸­æš‚ç•¥) è‡ªåŠ¨è¯†åˆ«å…³é”®å‰§æƒ…èŠ‚ç‚¹ã€‚
        2. **è§‚å¯Ÿæ”¶é›† (Observation Collection)**: è‡ªåŠ¨æå–å¯¹è§’è‰²çš„æ´å¯Ÿï¼Œç”Ÿæˆâ€œå¾…å®¡æ ¸å»ºè®®â€ã€‚
        3. **å…³ç³»æ¨æ¼” (Relationship Inference)**: æ ¹æ®äº’åŠ¨è‡ªåŠ¨æ›´æ–°è§’è‰²é—´çš„å…³ç³»å¼ºåº¦å’Œæƒ…æ„Ÿå€¾å‘ã€‚
        
        Args:
            db (Session): æ•°æ®åº“ä¼šè¯
            session_id (str): å½“å‰ä¼šè¯ID
            structured_data (dict): LLM åˆ†æå‡ºçš„ JSON æ•°æ®
        """
        # --- Pillar 2: Auto-generate Character Events (è‡ªåŠ¨ç”Ÿæˆè§’è‰²äº‹ä»¶) ---
        if db and "character_analysis" in structured_data:
            try:
                global_summary = structured_data.get("summary", "Deep Analysis Session")
                for char_data in structured_data["character_analysis"]:
                    char_name = char_data.get("name")
                    if not char_name:
                        continue
                        
                    # Find character ID by name
                    character = db.query(Character).filter(Character.name == char_name).first()
                    if character:
                        pass # å®é™…é€»è¾‘å¾…å®ç°ï¼Œç›®å‰ä¸ºå ä½ç¬¦
            except Exception as e:
                logger.error(f"Failed to auto-generate character events: {e}")

        # --- Pillar 2.5: Auto-collect Character Observations (è‡ªåŠ¨æ”¶é›†è§‚å¯Ÿå»ºè®®) ---
        # è¿™æ˜¯â€œåŠ¨æ€æ¡£æ¡ˆâ€çš„æ ¸å¿ƒæ¥æºã€‚ç³»ç»Ÿè‡ªåŠ¨å‘ç°è§’è‰²çš„æ–°ç‰¹å¾ï¼Œä½†ä¸ç›´æ¥ä¿®æ”¹æ¡£æ¡ˆï¼Œ
        # è€Œæ˜¯ç”Ÿæˆâ€œPending Observationsâ€ä¾›ç®¡ç†å‘˜å®¡æ ¸ã€‚
        if db and session_id and "character_observations" in structured_data:
            try:
                observations = structured_data["character_observations"]
                if observations:
                    character_observation_service.add_observations(db, session_id, observations)
                    logger.info(f"Processed {len(observations)} character observations")
            except Exception as e:
                logger.error(f"Failed to process character observations: {e}")

        # --- Pillar 3: Relationship Inference Engine (å…³ç³»æ¨æ¼”å¼•æ“) ---
        # è‡ªåŠ¨é‡åŒ–è§’è‰²é—´çš„äº’åŠ¨å½±å“ã€‚
        # strength_delta: å…³ç³»å¼ºåº¦çš„å˜åŒ– (å¦‚ +1 å˜å¾—æ›´ç´§å¯†, -1 å˜å¾—ç–è¿œ)
        # sentiment_delta: æƒ…æ„Ÿå€¾å‘çš„å˜åŒ– (å¦‚ +1 å˜å¾—æ›´å–œæ¬¢, -1 å˜å¾—åŒæ¶)
        if db and "relationship_updates" in structured_data:
            try:
                updates = structured_data["relationship_updates"]
                for update in updates:
                    source = update.get("source")
                    target = update.get("target")
                    s_delta = update.get("strength_delta", 0)
                    sent_delta = update.get("sentiment_delta", 0)
                    
                    if source and target:
                        character_service.update_relationship_state(
                            db, source, target, 
                            strength_delta=s_delta, 
                            sentiment_delta=sent_delta
                        )
                if updates:
                    logger.info(f"Processed {len(updates)} relationship updates")
            except Exception as e:
                logger.error(f"Failed to process relationship updates: {e}")

    async def deep_analyze(self, text: str, character_names: List[str], db: Session = None, session_id: str = None, history_context: List[dict] = None) -> Dict[str, Any]:
        """
        æ·±åº¦å¯¹è¯åˆ†æ (Deep Analysis).
        
        åŠŸèƒ½æè¿°:
        è°ƒç”¨ LLM å¯¹é•¿å¯¹è¯è¿›è¡Œæ·±åº¦å¿ƒç†å’Œæˆ˜ç•¥åˆ†æã€‚
        æ”¯æŒâ€œæ··åˆè¾“å‡ºæ¨¡å¼â€ï¼šåŒæ—¶è¿”å› Markdown æ ¼å¼çš„å¯è¯»æŠ¥å‘Šå’Œ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ã€‚
        
        ç¨³å®šæ€§æœºåˆ¶ (Reliability):
        - **ç†”æ–­æœºåˆ¶ (Circuit Breaker)**: å¦‚æœæ·±åº¦åˆ†æè°ƒç”¨å¤±è´¥ (å¦‚è¶…æ—¶æˆ– Token è¶…é™)ï¼Œ
          è‡ªåŠ¨é™çº§è°ƒç”¨ `quick_analyze`ï¼Œç¡®ä¿ç”¨æˆ·æ€»èƒ½è·å¾—åŸºç¡€ç»“æœã€‚
        
        Args:
            text (str): å¯¹è¯æ–‡æœ¬
            character_names (list): å·²çŸ¥è§’è‰²ååˆ—è¡¨ (è¾…åŠ© LLM è¯†åˆ«)
            db (Session, optional): æ•°æ®åº“ä¼šè¯ (ç”¨äºæŒä¹…åŒ–å‰¯ä½œç”¨)
            session_id (str, optional): ä¼šè¯ID
            history_context (list, optional): å†å²åˆ†ææ‘˜è¦åˆ—è¡¨ï¼Œç”¨äºç»¼åˆåˆ†æ
            
        Returns:
            dict: { "markdown_report": str, "structured_data": dict, ... }
        """
        start_time = time.time()
        
        # Load config (deep_analysis.primary)
        config = settings.PROMPTS.get("deep_analysis", {}).get("primary", {})
        prompt_template = config.get("prompt", "")
        temperature = config.get("temperature", 0.4)
        
        # Inject History Context if available
        history_text = ""
        if history_context:
            history_text = "\n\nã€å†å²åˆ†ææ‘˜è¦ (History Context)ã€‘:\n"
            for i, record in enumerate(history_context):
                ts = record.get("timestamp", "Unknown Time")
                summary = record.get("summary", "No summary")
                history_text += f"Records[{i+1}] ({ts}): {summary}\n"
            history_text += "\nè¯·ç»“åˆä¸Šè¿°å†å²ä¸Šä¸‹æ–‡ï¼Œå¯¹æœ¬æ¬¡å¯¹è¯è¿›è¡Œæ›´æ·±å…¥çš„è¿è´¯æ€§åˆ†æã€‚\n"

        prompt = prompt_template.format(
            text=history_text + text,
            character_names=", ".join(character_names)
        )

        
        # Call LLM - NO JSON ENFORCEMENT for mixed output (Markdown + JSON)
        # æˆ‘ä»¬å…è®¸ LLM è‡ªç”±è¾“å‡º Markdown æ–‡æœ¬ï¼Œå¹¶åœ¨å…¶ä¸­åµŒå…¥ ```json ä»£ç å—
        try:
            response = await llm_service.chat_completion(
                [{"role": "user", "content": prompt}],
                temperature=temperature
                # response_format={"type": "json_object"} # Removed to allow Markdown
            )
        except Exception as e:
            logger.error(f"Deep Analysis LLM call failed: {e}. Switching to Circuit Breaker mode.")
            # --- ç†”æ–­æœºåˆ¶ (Circuit Breaker) ---
            # è‡ªåŠ¨åˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼ (Fallback to Quick Analyze)
            fallback_result = await self.quick_analyze(text)
            fallback_result["markdown_report"] = f"### ğŸ›¡ï¸ ç†”æ–­æœºåˆ¶å·²è§¦å‘ (Circuit Breaker)\n\n> æ£€æµ‹åˆ°æ·±åº¦åˆ†ææœåŠ¡å“åº”å¼‚å¸¸ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢è‡³æ‘˜è¦æ¨¡å¼ã€‚\n\n" + fallback_result["markdown_report"]
            return fallback_result
        
        duration = time.time() - start_time
        logger.info(f"Deep Analysis completed in {duration:.2f}s. Input length: {len(text)}")
        
        if not response:
            return {"error": "LLM returned empty response"}
            
        # Parse Mixed Output: Extract JSON block
        # ä½¿ç”¨æ­£åˆ™æå– Markdown ä¸­çš„ JSON ä»£ç å—
        json_pattern = r"```json\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, response, re.DOTALL)
        
        if match:
            json_str = match.group(1)
            # The report is everything EXCEPT the JSON block
            # æŠ¥å‘Šå†…å®¹ = åŸå§‹å›å¤ - JSONå—
            markdown_report = response.replace(match.group(0), "").strip()
            try:
                structured_data = json.loads(json_str)
            except json.JSONDecodeError:
                structured_data = {"error": "Failed to parse JSON part"}
        else:
            # Fallback: Treat whole response as markdown, no structured data found
            # æœªå‘ç° JSON å—ï¼Œåˆ™è®¤ä¸ºå…¨æ˜¯æ–‡æœ¬æŠ¥å‘Š
            markdown_report = response
            structured_data = {}
            
        # Process Results using shared method (è§¦å‘å‰¯ä½œç”¨ï¼šç”Ÿæˆè§‚å¯Ÿã€æ›´æ–°å…³ç³»)
        if db and session_id:
            await self.process_analysis_results(db, session_id, structured_data)
            
        return {
            "markdown_report": markdown_report,
            "structured_data": structured_data,
            "metrics": {
                "duration": duration,
                "input_chars": len(text)
            }
        }

    async def quick_analyze(self, text: str) -> Dict[str, Any]:
        """
        Perform quick analysis (Degradation Mode).
        """
        start_time = time.time()
        
        config = settings.PROMPTS.get("deep_analysis", {}).get("quick_parse", {})
        prompt_template = config.get("prompt", "")
        temperature = config.get("temperature", 0.2)
        
        prompt = prompt_template.format(text=text)
        
        response = await llm_service.chat_completion(
            [{"role": "user", "content": prompt}],
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        duration = time.time() - start_time
        logger.info(f"Quick Analysis completed in {duration:.2f}s")
        
        try:
            structured_data = json.loads(response)
        except:
            structured_data = {"summary": "è§£æå¤±è´¥", "error": "Invalid JSON"}
            
        return {
            "markdown_report": f"### âš¡ å¿«é€Ÿåˆ†ææŠ¥å‘Š (é™çº§æ¨¡å¼)\n\n**æ‘˜è¦**: {structured_data.get('summary', 'æ— ')}\n\n*(æ³¨ï¼šç”±äºç³»ç»Ÿè´Ÿè½½æˆ–ç½‘ç»œåŸå› ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸ºå¿«é€Ÿæ¨¡å¼)*",
            "structured_data": structured_data,
            "mode": "quick",
            "metrics": {
                "duration": duration,
                "input_chars": len(text)
            }
        }

extraction_service = ExtractionService()
